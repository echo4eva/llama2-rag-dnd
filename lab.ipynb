{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "logging into hugging face so we can access the llama2! (very exciting)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token will not been saved to git credential helper. Pass `add_to_git_credential=True` if you want to set the git credential as well.\n",
      "Token is valid (permission: read).\n",
      "Your token has been saved to C:\\Users\\jerii\\.cache\\huggingface\\token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "from huggingface_hub import login\n",
    "login(token=os.getenv(\"TOKEN\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "make sure we're using gpu so we can use tensors!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.2.2+cu118\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.__version__)\n",
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "run the LLM with the prompt template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n",
      "model.safetensors.index.json: 100%|██████████| 26.8k/26.8k [00:00<00:00, 26.8MB/s]\n",
      "c:\\Users\\jerii\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\dndpdf-fhN5lhv7-py3.12\\Lib\\site-packages\\huggingface_hub\\file_download.py:149: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\jerii\\.cache\\huggingface\\hub\\models--meta-llama--Llama-2-7b-chat-hf. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "model-00001-of-00002.safetensors: 100%|██████████| 9.98G/9.98G [04:24<00:00, 37.8MB/s]\n",
      "model-00002-of-00002.safetensors: 100%|██████████| 3.50G/3.50G [01:28<00:00, 39.4MB/s]\n",
      "Downloading shards: 100%|██████████| 2/2 [05:53<00:00, 176.99s/it]\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:10<00:00,  5.05s/it]\n",
      "generation_config.json: 100%|██████████| 188/188 [00:00<00:00, 188kB/s]\n",
      "tokenizer_config.json: 100%|██████████| 1.62k/1.62k [00:00<?, ?B/s]\n",
      "tokenizer.model: 100%|██████████| 500k/500k [00:00<00:00, 10.4MB/s]\n",
      "tokenizer.json: 100%|██████████| 1.84M/1.84M [00:00<00:00, 9.87MB/s]\n",
      "special_tokens_map.json: 100%|██████████| 414/414 [00:00<00:00, 413kB/s]\n"
     ]
    }
   ],
   "source": [
    "from llama_index.llms.huggingface import HuggingFaceLLM\n",
    "from llama_index.core import PromptTemplate\n",
    "\n",
    "SYSTEM_PROMPT = \"\"\"You are an AI assistant that answers questions in a friendly manner, based on the given source documents. Here are some rules you always follow:\n",
    "- Generate human readable output, avoid creating output with gibberish text.\n",
    "- Generate only the requested output, don't include any other language before or after the requested output.\n",
    "- Never say thank you, that you are happy to help, that you are an AI agent, etc. Just answer directly.\n",
    "- Generate professional language typically used in business documents in North America.\n",
    "- Never generate offensive or foul language.\n",
    "\"\"\"\n",
    "\n",
    "model = \"meta-llama/Llama-2-7b-chat-hf\"\n",
    "query_wrapper_prompt = PromptTemplate(\n",
    "    \"[INST]<<SYS>>\\n\" + SYSTEM_PROMPT + \"<</SYS>>\\n\\n{query_str}[/INST] \"\n",
    ")\n",
    "\n",
    "llm = HuggingFaceLLM(\n",
    "    context_window=4096,\n",
    "    max_new_tokens=256,\n",
    "    generate_kwargs={\"temperature\": 0.0, \"do_sample\": False},\n",
    "    query_wrapper_prompt=query_wrapper_prompt,\n",
    "    tokenizer_name=model,\n",
    "    model_name=model,\n",
    "    device_map=\"auto\",\n",
    "    # change these settings below depending on your GPU\n",
    "    model_kwargs={\"torch_dtype\": torch.float16, \"load_in_4bit\": True},\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### loading documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "loading documents using `SimpleDirectoryReader` ([docs](https://docs.llamaindex.ai/en/stable/examples/data_connectors/simple_directory_reader/))\n",
    "\n",
    "this is kinda all that is needed bro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import SimpleDirectoryReader\n",
    "\n",
    "reader = SimpleDirectoryReader(input_dir=\"./data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "check if we have the file lol. i'm using dnd 5e which is 180 pages, so the `len(docs)` should return 180. :D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "180"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dnd_docs = reader.load_data()\n",
    "len(dnd_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "need to get embeddeding before indexing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "modules.json: 100%|██████████| 349/349 [00:00<00:00, 348kB/s]\n",
      "c:\\Users\\jerii\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\dndpdf-fhN5lhv7-py3.12\\Lib\\site-packages\\huggingface_hub\\file_download.py:149: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\jerii\\AppData\\Local\\llama_index\\models--BAAI--bge-small-en-v1.5. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "config_sentence_transformers.json: 100%|██████████| 124/124 [00:00<?, ?B/s] \n",
      "README.md: 100%|██████████| 94.8k/94.8k [00:00<00:00, 917kB/s]\n",
      "sentence_bert_config.json: 100%|██████████| 52.0/52.0 [00:00<?, ?B/s]\n",
      "config.json: 100%|██████████| 743/743 [00:00<?, ?B/s] \n",
      "model.safetensors: 100%|██████████| 133M/133M [00:04<00:00, 27.8MB/s] \n",
      "tokenizer_config.json: 100%|██████████| 366/366 [00:00<?, ?B/s] \n",
      "vocab.txt: 100%|██████████| 232k/232k [00:00<00:00, 2.35MB/s]\n",
      "tokenizer.json: 100%|██████████| 711k/711k [00:00<00:00, 4.42MB/s]\n",
      "special_tokens_map.json: 100%|██████████| 125/125 [00:00<?, ?B/s] \n",
      "1_Pooling/config.json: 100%|██████████| 190/190 [00:00<00:00, 189kB/s]\n"
     ]
    }
   ],
   "source": [
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "\n",
    "embed_model = HuggingFaceEmbedding(model_name=\"BAAI/bge-small-en-v1.5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we use `Settings` for LlamaIndex to set our LLM and embed model. *(look into this!)* we need embedding to index?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import Settings\n",
    "\n",
    "Settings.llm = llm\n",
    "Settings.embed_model = embed_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### indexing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parsing nodes: 100%|██████████| 180/180 [00:00<00:00, 241.30it/s]\n",
      "Generating embeddings: 100%|██████████| 320/320 [00:04<00:00, 68.89it/s]\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core import VectorStoreIndex\n",
    "\n",
    "index = VectorStoreIndex.from_documents(dnd_docs, show_progress=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### query"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "query the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_engine = index.as_query_engine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = query_engine.query(\"What are the classes and subclasses a player can play as?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the provided context and rules, a player can play as the following classes and subclasses:\n",
      "\n",
      "1. Cleric: A priestly champion who wields divine magic in service of a higher power. Subclasses include:\n",
      "\t* Channel Divinity: A cleric who can channel divine energy directly from their deity.\n",
      "\t* Life: A cleric who specializes in healing and restoration magic.\n",
      "\t* Light: A cleric who specializes in protective and beneficial magic.\n",
      "2. Fighter: A master of martial combat, skilled with a variety of weapons and armor. Subclasses include:\n",
      "\t* Battle Master: A fighter who excels in combat maneuvers and tactical thinking.\n",
      "\t* Frontline Fighter: A fighter who specializes in melee combat and taking damage for their allies.\n",
      "\t* Shield Master: A fighter who specializes in defensive magic and shields.\n",
      "3. Rogue: A scoundrel who uses stealth and trickery to overcome obstacles and enemies. Subclasses include:\n",
      "\t* Assassin: A rogue who specializes in killing and stealth.\n",
      "\t* Th\n"
     ]
    }
   ],
   "source": [
    "print(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dndpdf-fhN5lhv7-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
